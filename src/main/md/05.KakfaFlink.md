## 1) Kafka → Flink table (JSON value) + use Kafka record timestamp as `order_time`

```sql
CREATE TABLE orders_json (
  sequence_id BIGINT,
  order_id    STRING,
  price       DECIMAL(18,2),

  -- pull Kafka record timestamp from metadata
  order_time  TIMESTAMP_LTZ(3) METADATA FROM 'timestamp',

  -- optional: also expose partition/offset if you want
  kafka_partition INT     METADATA FROM 'partition' VIRTUAL,
  kafka_offset    BIGINT  METADATA FROM 'offset'    VIRTUAL
) WITH (
  'connector' = 'kafka',
  'topic' = 'orders-json',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'flink-orders-json',
  'scan.startup.mode' = 'earliest-offset',

  'format' = 'json',
  'json.ignore-parse-errors' = 'true'
);
```

---

## 2) Read JSON, compute something, write JSON to another Kafka topic (Kafka as sink)

```sql
CREATE TABLE orders_json_out (
  sequence_id BIGINT,
  order_id    STRING,
  price       DECIMAL(18,2),
  order_time  TIMESTAMP_LTZ(3)
) WITH (
  'connector' = 'kafka',
  'topic' = 'orders-json-out',
  'properties.bootstrap.servers' = 'localhost:9092',

  'format' = 'json'
);

INSERT INTO orders_json_out
SELECT
  sequence_id,
  order_id,
  price,
  order_time
FROM orders_json
WHERE price >= 100.00;
```

---

## 3) Kafka → Flink table using **Protobuf** (value is Protobuf)

Assume your Protobuf message is something like `com.example.Order` with fields:
`sequence_id`, `order_id`, `price`, and maybe `event_time`.

```sql
CREATE TABLE orders_pb (
  sequence_id BIGINT,
  order_id    STRING,
  price       DOUBLE,

  -- Kafka record timestamp
  order_time  TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'
) WITH (
  'connector' = 'kafka',
  'topic' = 'orders-protobuf',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'flink-orders-pb',
  'scan.startup.mode' = 'earliest-offset',

  'format' = 'protobuf',
  'protobuf.message-class-name' = 'com.example.Order'
);
```

---

## 4) Protobuf in, JSON out (format conversion inside Flink SQL)

```sql
CREATE TABLE orders_from_pb_to_json (
  sequence_id BIGINT,
  order_id    STRING,
  price       DOUBLE,
  order_time  TIMESTAMP_LTZ(3)
) WITH (
  'connector' = 'kafka',
  'topic' = 'orders-from-pb-to-json',
  'properties.bootstrap.servers' = 'localhost:9092',
  'format' = 'json'
);

INSERT INTO orders_from_pb_to_json
SELECT
  sequence_id,
  order_id,
  price,
  order_time
FROM orders_pb;
```

---

## 5) JSON key + JSON value (separately), still using Kafka metadata timestamp

This is useful if your **key** contains `order_id` and the **value** contains everything else.

```sql
CREATE TABLE orders_key_value_json (
  order_id    STRING,
  sequence_id BIGINT,
  price       DECIMAL(18,2),

  order_time  TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'
) WITH (
  'connector' = 'kafka',
  'topic' = 'orders-kv-json',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'flink-orders-kv',
  'scan.startup.mode' = 'earliest-offset',

  'key.format' = 'json',
  'key.fields' = 'order_id',

  'value.format' = 'json'
);
```

---

### Notes (so you don’t get surprised)

* `METADATA FROM 'timestamp'` uses **Kafka record timestamp** (CreateTime / LogAppendTime depending on broker/topic settings).
* Protobuf support depends on your Flink distribution + connector version; the exact option name is commonly `protobuf.message-class-name` (and sometimes slightly different across versions/distros).
* If you want **event-time watermarks** (real streaming windows), you’d usually add:

  * an event-time column (from payload or metadata),
  * plus a `WATERMARK FOR ... AS ...` clause.

If you tell me **your Flink version** and whether you’re using **Confluent Cloud / self-hosted Kafka**, I can tailor these `WITH (...)` properties exactly to your setup.
