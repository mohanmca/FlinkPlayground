## 1. From Flink code to running job

1. You write a Flink program using a **Flink API**.
2. That program acts as a **Flink client**.
3. When executed, the client:

   * Builds a **Job Graph**
   * Submits it to the **Job Manager**

---

## 2. Job Manager responsibilities

4. The **Job Manager**:

   * Receives the job graph
   * Finds or creates required resources
5. In containerized environments:

   * It spins up **Task Manager pods** as needed
6. It stays active during execution and:

   * Coordinates **checkpointing**
   * Detects failures
   * Restarts failed Task Managers

---

## 3. Task Managers & parallelism

7. Each **Task Manager**:

   * Provides multiple **task slots**
8. Each task slot:

   * Runs **one parallel instance** of the job graph
9. Task Managers:

   * Execute the actual work
   * Pull data from **sources**
   * Transform data
   * Exchange data with other Task Managers (shuffle/repartition)
   * Push results to **sinks**

---

## 4. One runtime, two execution modes

10. Flink supports **both streaming and batch**
11. You:

* Write the code **once**
* Runtime decides **streaming vs batch**

12. In Flink:

* **Batch = special case of streaming**
* Batch mode unlocks extra optimizations

---

## 5. Bounded vs unbounded streams

13. **Batch mode**:

* Requires **bounded streams**

14. **Streaming mode**:

* Handles **unbounded streams**
* Cannot wait for “end of data”

---

## 6. Streaming execution characteristics

15. Streaming jobs:

* Must run **continuously**
* Aim for **low end-to-end latency**

16. Each event:

* Is processed as it arrives
* Could be the *last event*

17. Results are produced:

* Incrementally (per event)
* Or periodically (via timers)

---

## 7. Batch execution characteristics

18. Batch jobs:

* Run until completion
* Produce results **at the end**

19. Runtime optimizations in batch:

* Pipeline stages can run **sequentially**
* Less resource usage than streaming

20. Example optimization:

* Filter all data first
* Then pass filtered data to next stage

21. Additional batch-only optimizations:

* **Pre-sorting inputs**
* Makes operations like joins cheaper

---

## 8. State & buffering differences

22. Streaming:

* Events may need buffering
* Buffers must be stored in **durable state**
* Needed for failure recovery

23. Batch:

* No need for durable buffering
* Can restart from scratch

---

## 9. Failure recovery

24. Batch recovery:

* Restart the job from the beginning

25. Streaming recovery:

* Resume from a **recent snapshot**
* Must minimize downtime
* Must avoid excessive reprocessing

---

## 10. Exactly-once guarantees

26. Flink provides **exactly-once semantics**:

* In both batch and streaming

27. Streaming exactly-once:

* More complex
* Requires fast, consistent recovery

---

## 11. Why stream processing is worth it

28. Stream processing:

* Is **harder** to support than batch
* Places stronger demands on the runtime

29. But it enables:

* **Low-latency use cases**
* Real-time decisions

---

## 12. Use case comparison

30. Use cases that **require streaming**:

* Fraud detection
* Data center monitoring
* Any low-latency system

31. Batch latency:

* Makes these use cases impractical

---

## 13. Final takeaway

32. Stream processing is **strictly more powerful**
33. Any batch use case:

* Can be handled by a stream processor

34. But:

* Batch mode is still valuable
* Because it is **more efficient** when applicable