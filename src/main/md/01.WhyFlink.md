## 1. [What stream processing is and why it matters](https://speaking.gamov.io/mrxzS1/present)

* **Stream processing** is about handling events *as they happen* in real time.
* Modern users expect **instant access to data and immediate reactions**:

  * Credit-card fraud should trigger alerts immediately.
  * Online orders should provide accurate delivery estimates.
  * Any disruption to delivery should generate timely notifications.
* These expectations are driving companies toward **real-time, automated, software-defined business operations**.

---

## 2. Why Apache Flink is important

* **Apache Flink** is a powerful framework for:

  * Connecting data sources
  * Enriching data
  * Processing data in real time
* Real-time business operations were **not feasible before event streaming platforms** like Kafka and Flink.
* Event streaming is transforming traditional industries:

  * Banking
  * Telecommunications
  * Retail
* It also enables **new business models**, such as ride-sharing.
* Flink is becoming an **integral part of how many companies operate their businesses**.

---

## 3. Why you should consider using Flink

* Flink has a **very active and supportive open-source community**:

  * Mailing lists and forums are among the most active in the Apache ecosystem.
* Flink is **battle-tested at massive scale**.
* Companies that have publicly shared success stories include:

  * Netflix
  * Alibaba
  * Uber
  * Goldman Sachs
* Flink provides **expressive APIs**:

  * Java
  * Python
  * SQL
* Flink supports **both stream and batch processing**, making it highly flexible for many use cases.

---

## 4. The four core ideas behind Flink

The course is organized around four foundational concepts:

1. **Streaming**
2. **State**
3. **Time**
4. **Snapshots (for fault tolerance and recovery)**

* Understanding how these ideas fit together is essential to understanding how Flink works.
* This particular video focuses **only on streaming**.

---

## 5. What event streaming means (technically)

* Event streaming is the practice of **capturing events in real time as they occur**.
* Examples of events:

  * Orders
  * Shipments
  * Downloads
  * Clicks
* Business events can *always* be streamed.
* Event streams can be:

  * **Unbounded**: extend indefinitely into the future and are processed continuously.
  * **Bounded**: stored and reprocessed later.
* Reprocessing historical data is simply a **special case of streaming** where the stream has a defined start and end time.

---

## 6. Structure of a Flink application

* A Flink application:

  * **Consumes data** from one or more **sources**
  * **Produces data** to one or more **sinks**
* Sources and sinks can be:

  * Messaging systems (e.g., Kafka)
  * Files
  * Databases
  * Services or applications
* The focus of stream processing is the **logic in the middle**, between sources and sinks.
* Developers write this logic using Flink APIs.
* Flink executes this logic in a **Flink cluster**.

---

## 7. Core terminology

* A running Flink application is called a **Job**.
* The processing pipeline is called the **Job Graph**.
* The Job Graph consists of:

  * **Nodes** → processing steps
  * **Operators** → execute transformations on event streams
* Operators are connected by edges that show how data flows.
* The Job Graph is always a **directed acyclic graph (DAG)**.
* Data always flows **from sources toward sinks**.

---

## 8. Parallelism and scalability

* Stream processing in Flink is **parallel by design**.
* Event streams are partitioned into **parallel sub-streams**.
* Each sub-stream can be processed independently.
* This partitioning is **crucial for scalability**.
* Parallel operators:

  * Share nothing
  * Run at full speed
* Input streams are often **already partitioned upstream** before Flink consumes them.

---

## 9. Example: forwarding, filtering, and repartitioning

* In the example:

  * Input data is initially partitioned by **shape**.
* First operator:

  * Collects data from the source
  * Forwards it downstream
  * Forwarding is highly optimized and efficient.
* Second operator:

  * Filters out **orange** events
  * Re-partitions the stream by **color** instead of shape
* This shuffle groups:

  * All yellow events together
  * All blue events together
* Grouping enables counting all events of the same color.

---

## 10. Cost of shuffling and rebalancing

* **Shuffling (re-partitioning)** is expensive:

  * Events must be serialized
  * Events may need to cross the network
* Counting results:

  * Yellow count increases: 1 → 2 → 3 → 4
  * Blue count increases: 1 → 2 → 3
* The example reduces sink parallelism from two to one using **rebalancing**:

  * Rebalancing redistributes events round-robin
  * Streams are merged at the sink
* This reduction was done **only for demonstration purposes**.
* In practice:

  * Sinks usually match the parallelism of the job
  * Rebalancing is expensive and should be avoided unless necessary

---

## 11. Other stream processing patterns in Flink

Beyond forwarding, shuffling, and rebalancing, Flink supports:

* Broadcasting streams (distributing data cluster-wide)
* Joining streams (data enrichment)
* Many additional processing patterns

---

## 12. Expressing the example in Flink SQL

* The entire example can be written using **Flink SQL**.
* Flink SQL transforms SQL statements into a Flink application.
* SQL components map to the job graph:

  * `INSERT INTO results` → defines the sink
  * `FROM events` → defines the source
  * `WHERE color != orange` → filtering operator
  * `GROUP BY color` and `COUNT` → re-partitioning and counting logic
* Flink automatically generates the streaming job to execute this query.
* More details on Flink SQL are covered later in the course.
